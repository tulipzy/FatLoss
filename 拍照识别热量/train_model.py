import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, random_split
import os

# åªå®šä¹‰å˜é‡å’Œå‡½æ•°ï¼Œä¸æ‰§è¡Œå®é™…è®­ç»ƒé€»è¾‘
DATA_DIR = r'C:\Users\86151\Downloads\food-101\food-101\images'
BATCH_SIZE = 32
NUM_EPOCHS = 20
NUM_CLASSES = 101
LR = 1e-43

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# æ•°æ®å¢å¼ºå’Œé¢„å¤„ç†
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.2, 0.2, 0.2),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

val_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])


# è®­ç»ƒå’ŒéªŒè¯å‡½æ•°
def train_epoch(model, train_loader, criterion, optimizer, device):
    model.train()
    running_loss, correct, total = 0.0, 0, 0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        outputs = model(images)
        loss = criterion(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * images.size(0)
        _, preds = outputs.max(1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    return running_loss / total, 100. * correct / total


def validate(model, val_loader, criterion, device):
    model.eval()
    running_loss, correct, total = 0.0, 0, 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)

            running_loss += loss.item() * images.size(0)
            _, preds = outputs.max(1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

    return running_loss / total, 100. * correct / total


# ä¸»ç¨‹åºå…¥å£ï¼ˆå…³é”®ä¿®æ”¹ï¼‰
if __name__ == '__main__':
    # åœ¨ Windows ä¸Šä½¿ç”¨å¤šè¿›ç¨‹æ—¶éœ€è¦çš„å†»ç»“æ”¯æŒ
    import multiprocessing

    multiprocessing.freeze_support()

    # åŠ è½½å®Œæ•´æ•°æ®é›†
    full_dataset = datasets.ImageFolder(DATA_DIR, transform=train_transform)

    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])

    # ä¸ºéªŒè¯é›†è®¾ç½®å•ç‹¬çš„ transform
    val_dataset.dataset.transform = val_transform

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆWindows ä¸Š num_workers ä¸å®œè¿‡å¤§ï¼‰
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=2,  # å‡å°‘ workers æ•°é‡ï¼Œé¿å… Windows èµ„æºé—®é¢˜
        pin_memory=DEVICE.type == 'cuda'  # åªæœ‰ GPU æ—¶æ‰å¯ç”¨ pin_memory
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=2,
        pin_memory=DEVICE.type == 'cuda'
    )

    # æ‰“å°æ•°æ®é›†ä¿¡æ¯
    print(f"æ•°æ®é›†æ€»æ ·æœ¬æ•°: {len(full_dataset)}")
    print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
    print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_dataset)}")
    print(f"ç±»åˆ«æ•°: {len(full_dataset.classes)}")
    print(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")

    # MobileNetV3 æ¨¡å‹
    model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)  # ä¿®æ­£ weights å‚æ•°
    model.classifier[3] = nn.Linear(model.classifier[3].in_features, NUM_CLASSES)
    model = model.to(DEVICE)

    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)

    # åˆ›å»ºä¿å­˜æ¨¡å‹çš„ç›®å½•
    os.makedirs('checkpoints', exist_ok=True)

    # è®­ç»ƒå¾ªç¯
    best_val_acc = 0.0
    for epoch in range(NUM_EPOCHS):
        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)
        val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)

        scheduler.step()

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'checkpoints/mobilenetv3_food101_best.pth')
            print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼ŒéªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")

        print(f"ğŸ“¦ Epoch {epoch + 1}/{NUM_EPOCHS} | "
              f"è®­ç»ƒæŸå¤±: {train_loss:.4f} | è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}% | "
              f"éªŒè¯æŸå¤±: {val_loss:.4f} | éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}% | "
              f"å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(model.state_dict(), 'checkpoints/mobilenetv3_food101_final.pth')
    print("âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ï¼šcheckpoints/mobilenetv3_food101_final.pth")
    print(f"ğŸ¥‡ æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")